{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from dataset import get_twit_company_dataloaders, get_twit_sentiment_dataloaders, get_twit_company_sentiment_dataloaders\n",
    "from model import LSTMTwitClassifier\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# text, label = next(iter(dataloader_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ars86\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet ignored due to unreadability: Поиск от \n",
      "Tweet ignored due to unreadability: Новите \n",
      "Tweet ignored due to unreadability: 看見 \n",
      "Tweet ignored due to unreadability: نظام جديد .. و جهاز جديد شكراً جزيلاً \n",
      "Tweet ignored due to unreadability: الجهاز الجديد عجيب   \n",
      "Tweet ignored due to unreadability: يبدو ان طفرة الاجهزة الالكترونية القادمة ستكون بقيادة موتورولا ،، لاسيم بعد استحواذ قوقل عليها.   \n",
      "Tweet ignored due to unreadability: Με συγχισες \n",
      "Tweet ignored due to unreadability: На сайте \n",
      "Tweet ignored due to unreadability: Настоящий твиттерянин как только попадает в толпу стремиться тут же как можно быстрее попасть в \n",
      "Tweet ignored due to unreadability: Доброе утро \n",
      "Tweet ignored due to unreadability: 【\n",
      "Tweet ignored due to unreadability: رقم الفلو والفلورز والتويتات  للبيع لاعلى سعر \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ars86\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet ignored due to unreadability: قال الرئيس التنفيذي لشركة \n",
      "Tweet ignored due to unreadability: Улучшим продукты компании \n",
      "Tweet ignored due to unreadability: نفسي يوم يعدي علي تويتر من غير مشاكل فنية \n",
      "Tweet ignored due to unreadability: ツイッター検索 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ars86\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "wandb: Currently logged in as: ars860 (use `wandb login --relogin` to force relogin)\n",
      "wandb: wandb version 0.12.5 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.29<br/>\n                Syncing run <strong style=\"color:#cdcd00\">trim-water-38</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/ars860/text2company_twit_classification\" target=\"_blank\">https://wandb.ai/ars860/text2company_twit_classification</a><br/>\n                Run page: <a href=\"https://wandb.ai/ars860/text2company_twit_classification/runs/3plfhkgk\" target=\"_blank\">https://wandb.ai/ars860/text2company_twit_classification/runs/3plfhkgk</a><br/>\n                Run data is saved locally in <code>E:\\acady\\learning\\sma\\twit_classifier\\wandb\\run-20211027_182134-3plfhkgk</code><br/><br/>\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, iter: 100/3401, mean loss: 0.2360259737703018\n",
      "Epoch 1/50, iter: 200/3401, mean loss: 0.00916188510862412\n",
      "Epoch 1/50, iter: 300/3401, mean loss: 0.006217722113397031\n",
      "Epoch 1/50, iter: 400/3401, mean loss: 0.0011987403784223716\n",
      "Epoch 1/50, iter: 500/3401, mean loss: 0.01061335138359027\n",
      "Epoch 1/50, iter: 600/3401, mean loss: 0.002946282478487774\n",
      "Epoch 1/50, iter: 700/3401, mean loss: 0.0004199304972598839\n",
      "Epoch 1/50, iter: 800/3401, mean loss: 0.00040287758041813505\n",
      "Epoch 1/50, iter: 900/3401, mean loss: 0.0014199040793641871\n",
      "Epoch 1/50, iter: 1000/3401, mean loss: 0.6121180474207768\n",
      "Epoch 1/50, iter: 1100/3401, mean loss: 0.23033541007433087\n",
      "Epoch 1/50, iter: 1200/3401, mean loss: 0.037400278793647886\n",
      "Epoch 1/50, iter: 1300/3401, mean loss: 0.01491446694766637\n",
      "Epoch 1/50, iter: 1400/3401, mean loss: 0.01240120724105509\n",
      "Epoch 1/50, iter: 1500/3401, mean loss: 0.02042275810279534\n",
      "Epoch 1/50, iter: 1600/3401, mean loss: 0.03190078524276032\n",
      "Epoch 1/50, iter: 1700/3401, mean loss: 0.017810509702540004\n",
      "Epoch 1/50, iter: 1800/3401, mean loss: 0.8952598223691166\n",
      "Epoch 1/50, iter: 1900/3401, mean loss: 0.1835948030068539\n",
      "Epoch 1/50, iter: 2000/3401, mean loss: 0.03890507179487031\n",
      "Epoch 1/50, iter: 2100/3401, mean loss: 0.03443729612568859\n",
      "Epoch 1/50, iter: 2200/3401, mean loss: 0.043506896678591145\n",
      "Epoch 1/50, iter: 2300/3401, mean loss: 0.015454980322283518\n",
      "Epoch 1/50, iter: 2400/3401, mean loss: 0.01489364980123355\n",
      "Epoch 1/50, iter: 2500/3401, mean loss: 0.02649899335923692\n",
      "Epoch 1/50, iter: 2600/3401, mean loss: 1.1493181006870328\n",
      "Epoch 1/50, iter: 2700/3401, mean loss: 0.1532080471329391\n",
      "Epoch 1/50, iter: 2800/3401, mean loss: 0.03629406840540469\n",
      "Epoch 1/50, iter: 2900/3401, mean loss: 0.032536467898462436\n",
      "Epoch 1/50, iter: 3000/3401, mean loss: 0.032777485081751365\n",
      "Epoch 1/50, iter: 3100/3401, mean loss: 0.041122528141131626\n",
      "Epoch 1/50, iter: 3200/3401, mean loss: 0.027553197526322036\n",
      "Epoch 1/50, iter: 3300/3401, mean loss: 0.01733863488481802\n",
      "Epoch 1/50, iter: 3400/3401, mean loss: 0.008872374593788663\n",
      "Epoch 2/50, iter: 100/3401, mean loss: 1.1771554429456592\n",
      "Epoch 2/50, iter: 200/3401, mean loss: 0.04376394775463268\n",
      "Epoch 2/50, iter: 300/3401, mean loss: 0.01467202342231758\n",
      "Epoch 2/50, iter: 400/3401, mean loss: 0.006841983639751561\n",
      "Epoch 2/50, iter: 500/3401, mean loss: 0.006221691961109173\n",
      "Epoch 2/50, iter: 600/3401, mean loss: 0.004367852539435262\n",
      "Epoch 2/50, iter: 700/3401, mean loss: 0.002291096927256149\n",
      "Epoch 2/50, iter: 800/3401, mean loss: 0.002665333354307222\n",
      "Epoch 2/50, iter: 900/3401, mean loss: 0.0026485892466735095\n",
      "Epoch 2/50, iter: 1000/3401, mean loss: 0.49614722955760954\n",
      "Epoch 2/50, iter: 1100/3401, mean loss: 0.40886019330471757\n",
      "Epoch 2/50, iter: 1200/3401, mean loss: 0.04955967724090442\n",
      "Epoch 2/50, iter: 1300/3401, mean loss: 0.019993636441067792\n",
      "Epoch 2/50, iter: 1400/3401, mean loss: 0.011306794899865054\n",
      "Epoch 2/50, iter: 1500/3401, mean loss: 0.010719536928809248\n",
      "Epoch 2/50, iter: 1600/3401, mean loss: 0.0176645174328587\n",
      "Epoch 2/50, iter: 1700/3401, mean loss: 0.013432392284885282\n",
      "Epoch 2/50, iter: 1800/3401, mean loss: 0.7587093494155852\n",
      "Epoch 2/50, iter: 1900/3401, mean loss: 0.22910556538961827\n",
      "Epoch 2/50, iter: 2000/3401, mean loss: 0.03142296564590651\n",
      "Epoch 2/50, iter: 2100/3401, mean loss: 0.02042183857949567\n",
      "Epoch 2/50, iter: 2200/3401, mean loss: 0.025293957448593574\n",
      "Epoch 2/50, iter: 2300/3401, mean loss: 0.006329827472873148\n",
      "Epoch 2/50, iter: 2400/3401, mean loss: 0.008565402020976762\n",
      "Epoch 2/50, iter: 2500/3401, mean loss: 0.01609988028096268\n",
      "Epoch 2/50, iter: 2600/3401, mean loss: 0.9192659624375483\n",
      "Epoch 2/50, iter: 2700/3401, mean loss: 0.1393081080878619\n",
      "Epoch 2/50, iter: 2800/3401, mean loss: 0.021408814792521297\n",
      "Epoch 2/50, iter: 2900/3401, mean loss: 0.017962256798346062\n",
      "Epoch 2/50, iter: 3000/3401, mean loss: 0.018995515959322803\n",
      "Epoch 2/50, iter: 3100/3401, mean loss: 0.01672335663216472\n",
      "Epoch 2/50, iter: 3200/3401, mean loss: 0.01649842505761626\n",
      "Epoch 2/50, iter: 3300/3401, mean loss: 0.009434706969955187\n",
      "Epoch 2/50, iter: 3400/3401, mean loss: 0.005992878116094289\n",
      "Epoch 3/50, iter: 100/3401, mean loss: 1.0859889319725335\n",
      "Epoch 3/50, iter: 200/3401, mean loss: 0.040090415244922045\n",
      "Epoch 3/50, iter: 300/3401, mean loss: 0.012112659149861428\n",
      "Epoch 3/50, iter: 400/3401, mean loss: 0.004412547271203948\n",
      "Epoch 3/50, iter: 500/3401, mean loss: 0.0071084234863519665\n",
      "Epoch 3/50, iter: 600/3401, mean loss: 0.0064529558388676375\n",
      "Epoch 3/50, iter: 700/3401, mean loss: 0.0022534678534066187\n",
      "Epoch 3/50, iter: 800/3401, mean loss: 0.001575747164497443\n",
      "Epoch 3/50, iter: 900/3401, mean loss: 0.0017905106634680124\n",
      "Epoch 3/50, iter: 1000/3401, mean loss: 0.5367663691517828\n",
      "Epoch 3/50, iter: 1100/3401, mean loss: 0.4329372573364526\n",
      "Epoch 3/50, iter: 1200/3401, mean loss: 0.06076217364287004\n",
      "Epoch 3/50, iter: 1300/3401, mean loss: 0.022887555129709653\n",
      "Epoch 3/50, iter: 1400/3401, mean loss: 0.017741828748767147\n",
      "Epoch 3/50, iter: 1500/3401, mean loss: 0.015219363112591963\n",
      "Epoch 3/50, iter: 1600/3401, mean loss: 0.03238945282269242\n",
      "Epoch 3/50, iter: 1700/3401, mean loss: 0.01626819943880946\n",
      "Epoch 3/50, iter: 1800/3401, mean loss: 0.7761313829279607\n",
      "Epoch 3/50, iter: 1900/3401, mean loss: 0.2641628862125799\n",
      "Epoch 3/50, iter: 2000/3401, mean loss: 0.07528159987763501\n",
      "Epoch 3/50, iter: 2100/3401, mean loss: 0.0482723228391842\n",
      "Epoch 3/50, iter: 2200/3401, mean loss: 0.03631756914495782\n",
      "Epoch 3/50, iter: 2300/3401, mean loss: 0.01322226699150633\n",
      "Epoch 3/50, iter: 2400/3401, mean loss: 0.013447920622675157\n",
      "Epoch 3/50, iter: 2500/3401, mean loss: 0.022452554106039316\n",
      "Epoch 3/50, iter: 2600/3401, mean loss: 0.9725956039652647\n",
      "Epoch 3/50, iter: 2700/3401, mean loss: 0.2283059371681884\n",
      "Epoch 3/50, iter: 2800/3401, mean loss: 0.045079902878496794\n",
      "Epoch 3/50, iter: 2900/3401, mean loss: 0.04247788608598057\n",
      "Epoch 3/50, iter: 3000/3401, mean loss: 0.04000698171221302\n",
      "Epoch 3/50, iter: 3100/3401, mean loss: 0.04091830555967135\n",
      "Epoch 3/50, iter: 3200/3401, mean loss: 0.03513077975560009\n",
      "Epoch 3/50, iter: 3300/3401, mean loss: 0.01804646106504606\n",
      "Epoch 3/50, iter: 3400/3401, mean loss: 0.010233197057341386\n",
      "Epoch 4/50, iter: 100/3401, mean loss: 0.9138403425714933\n",
      "Epoch 4/50, iter: 200/3401, mean loss: 0.023834206242463553\n",
      "Epoch 4/50, iter: 300/3401, mean loss: 0.020895384383038618\n",
      "Epoch 4/50, iter: 400/3401, mean loss: 0.005748679193020507\n",
      "Epoch 4/50, iter: 500/3401, mean loss: 0.0247269054014032\n",
      "Epoch 4/50, iter: 600/3401, mean loss: 0.01177372839056261\n",
      "Epoch 4/50, iter: 700/3401, mean loss: 0.0023153310504130786\n",
      "Epoch 4/50, iter: 800/3401, mean loss: 0.0020380081049916044\n",
      "Epoch 4/50, iter: 900/3401, mean loss: 0.00673324651990697\n",
      "Epoch 4/50, iter: 1000/3401, mean loss: 0.47891539363432456\n",
      "Epoch 4/50, iter: 1100/3401, mean loss: 0.3613556185550988\n",
      "Epoch 4/50, iter: 1200/3401, mean loss: 0.08366769512416795\n",
      "Epoch 4/50, iter: 1300/3401, mean loss: 0.02970353547891136\n",
      "Epoch 4/50, iter: 1400/3401, mean loss: 0.04326340205901943\n",
      "Epoch 4/50, iter: 1500/3401, mean loss: 0.026783036495689885\n",
      "Epoch 4/50, iter: 1600/3401, mean loss: 0.06584629921865598\n",
      "Epoch 4/50, iter: 1700/3401, mean loss: 0.03197416057119881\n",
      "Epoch 4/50, iter: 1800/3401, mean loss: 0.7432765330309848\n",
      "Epoch 4/50, iter: 1900/3401, mean loss: 0.22694631341844798\n",
      "Epoch 4/50, iter: 2000/3401, mean loss: 0.08621084841201082\n",
      "Epoch 4/50, iter: 2100/3401, mean loss: 0.06763448762125336\n",
      "Epoch 4/50, iter: 2200/3401, mean loss: 0.05567471932125045\n",
      "Epoch 4/50, iter: 2300/3401, mean loss: 0.027576489874190882\n",
      "Epoch 4/50, iter: 2400/3401, mean loss: 0.03920511058779084\n",
      "Epoch 4/50, iter: 2500/3401, mean loss: 0.0373901809293784\n",
      "Epoch 4/50, iter: 2600/3401, mean loss: 0.980343285480194\n",
      "Epoch 4/50, iter: 2700/3401, mean loss: 0.2027861266164109\n",
      "Epoch 4/50, iter: 2800/3401, mean loss: 0.04252424723585136\n",
      "Epoch 4/50, iter: 2900/3401, mean loss: 0.040412410523422294\n",
      "Epoch 4/50, iter: 3000/3401, mean loss: 0.0412298495270079\n",
      "Epoch 4/50, iter: 3100/3401, mean loss: 0.03898489719766076\n",
      "Epoch 4/50, iter: 3200/3401, mean loss: 0.03428449034265214\n",
      "Epoch 4/50, iter: 3300/3401, mean loss: 0.021607650078522056\n",
      "Epoch 4/50, iter: 3400/3401, mean loss: 0.013014012979110702\n",
      "Epoch 5/50, iter: 100/3401, mean loss: 0.8929293214902282\n",
      "Epoch 5/50, iter: 200/3401, mean loss: 0.042508551613427696\n",
      "Epoch 5/50, iter: 300/3401, mean loss: 0.016251628482714296\n",
      "Epoch 5/50, iter: 400/3401, mean loss: 0.011885668680188246\n",
      "Epoch 5/50, iter: 500/3401, mean loss: 0.01942956984130433\n",
      "Epoch 5/50, iter: 600/3401, mean loss: 0.00938806067504629\n",
      "Epoch 5/50, iter: 700/3401, mean loss: 0.003832926568866242\n",
      "Epoch 5/50, iter: 800/3401, mean loss: 0.0042273783534983525\n",
      "Epoch 5/50, iter: 900/3401, mean loss: 0.003387640855817153\n",
      "Epoch 5/50, iter: 1000/3401, mean loss: 0.4896351120132476\n",
      "Epoch 5/50, iter: 1100/3401, mean loss: 0.42429079858586194\n",
      "Epoch 5/50, iter: 1200/3401, mean loss: 0.08050999099039473\n",
      "Epoch 5/50, iter: 1300/3401, mean loss: 0.027515043402963783\n",
      "Epoch 5/50, iter: 1400/3401, mean loss: 0.01691298762947554\n",
      "Epoch 5/50, iter: 1500/3401, mean loss: 0.012395223143466865\n",
      "Epoch 5/50, iter: 1600/3401, mean loss: 0.03420517138558352\n",
      "Epoch 5/50, iter: 1700/3401, mean loss: 0.018531887390454357\n",
      "Epoch 5/50, iter: 1800/3401, mean loss: 0.6019383253261822\n",
      "Epoch 5/50, iter: 1900/3401, mean loss: 0.2344122231961228\n",
      "Epoch 5/50, iter: 2000/3401, mean loss: 0.0737777284590993\n",
      "Epoch 5/50, iter: 2100/3401, mean loss: 0.05138531871867599\n",
      "Epoch 5/50, iter: 2200/3401, mean loss: 0.033455011003970865\n",
      "Epoch 5/50, iter: 2300/3401, mean loss: 0.024473128844692837\n",
      "Epoch 5/50, iter: 2400/3401, mean loss: 0.01594247325683682\n",
      "Epoch 5/50, iter: 2500/3401, mean loss: 0.021233560337905147\n",
      "Epoch 5/50, iter: 2600/3401, mean loss: 0.8700623243576456\n",
      "Epoch 5/50, iter: 2700/3401, mean loss: 0.24570238317595794\n",
      "Epoch 5/50, iter: 2800/3401, mean loss: 0.04902688266069163\n",
      "Epoch 5/50, iter: 2900/3401, mean loss: 0.039906371379620394\n",
      "Epoch 5/50, iter: 3000/3401, mean loss: 0.03508661950007081\n",
      "Epoch 5/50, iter: 3100/3401, mean loss: 0.030605492539616534\n",
      "Epoch 5/50, iter: 3200/3401, mean loss: 0.015798552457126788\n",
      "Epoch 5/50, iter: 3300/3401, mean loss: 0.013839485854268786\n",
      "Epoch 5/50, iter: 3400/3401, mean loss: 0.009356927346502744\n",
      "Epoch 6/50, iter: 100/3401, mean loss: 1.188084156215191\n",
      "Epoch 6/50, iter: 200/3401, mean loss: 0.09262814443558455\n",
      "Epoch 6/50, iter: 300/3401, mean loss: 0.02771542696165852\n",
      "Epoch 6/50, iter: 400/3401, mean loss: 0.014596617276256439\n",
      "Epoch 6/50, iter: 500/3401, mean loss: 0.013486158979794709\n",
      "Epoch 6/50, iter: 600/3401, mean loss: 0.008428579755418468\n",
      "Epoch 6/50, iter: 700/3401, mean loss: 0.004158568681777979\n",
      "Epoch 6/50, iter: 800/3401, mean loss: 0.003472918879633653\n",
      "Epoch 6/50, iter: 900/3401, mean loss: 0.0032555519619199913\n",
      "Epoch 6/50, iter: 1000/3401, mean loss: 0.5038154655954713\n",
      "Epoch 6/50, iter: 1100/3401, mean loss: 0.5805365550518036\n",
      "Epoch 6/50, iter: 1200/3401, mean loss: 0.10045100048766471\n",
      "Epoch 6/50, iter: 1300/3401, mean loss: 0.03005691786413081\n",
      "Epoch 6/50, iter: 1400/3401, mean loss: 0.023437286148837302\n",
      "Epoch 6/50, iter: 1500/3401, mean loss: 0.01518000485295488\n",
      "Epoch 6/50, iter: 1600/3401, mean loss: 0.02630164091988263\n",
      "Epoch 6/50, iter: 1700/3401, mean loss: 0.012837689753068844\n",
      "Epoch 6/50, iter: 1800/3401, mean loss: 0.6447157827539559\n",
      "Epoch 6/50, iter: 1900/3401, mean loss: 0.346954446695745\n",
      "Epoch 6/50, iter: 2000/3401, mean loss: 0.0649938991270028\n",
      "Epoch 6/50, iter: 2100/3401, mean loss: 0.034719865133374696\n",
      "Epoch 6/50, iter: 2200/3401, mean loss: 0.03190545238350751\n",
      "Epoch 6/50, iter: 2300/3401, mean loss: 0.015992865114822053\n",
      "Epoch 6/50, iter: 2400/3401, mean loss: 0.009360145926184487\n",
      "Epoch 6/50, iter: 2500/3401, mean loss: 0.013836204309700405\n",
      "Epoch 6/50, iter: 2600/3401, mean loss: 0.8112728671797231\n",
      "Epoch 6/50, iter: 2700/3401, mean loss: 0.1988459801953286\n",
      "Epoch 6/50, iter: 2800/3401, mean loss: 0.04535630591097288\n",
      "Epoch 6/50, iter: 2900/3401, mean loss: 0.024576912200427615\n",
      "Epoch 6/50, iter: 3000/3401, mean loss: 0.022583962480421178\n",
      "Epoch 6/50, iter: 3100/3401, mean loss: 0.020464711733548028\n",
      "Epoch 6/50, iter: 3200/3401, mean loss: 0.01556293582823855\n",
      "Epoch 6/50, iter: 3300/3401, mean loss: 0.008585340745430586\n",
      "Epoch 6/50, iter: 3400/3401, mean loss: 0.008896366394544657\n",
      "Epoch 7/50, iter: 100/3401, mean loss: 0.8905892577953637\n",
      "Epoch 7/50, iter: 200/3401, mean loss: 0.06764209528453648\n",
      "Epoch 7/50, iter: 300/3401, mean loss: 0.030287710241391323\n",
      "Epoch 7/50, iter: 400/3401, mean loss: 0.008233124246107764\n",
      "Epoch 7/50, iter: 500/3401, mean loss: 0.012923969199400745\n",
      "Epoch 7/50, iter: 600/3401, mean loss: 0.008792588972592058\n",
      "Epoch 7/50, iter: 700/3401, mean loss: 0.002542764805589286\n",
      "Epoch 7/50, iter: 800/3401, mean loss: 0.002363087187841302\n",
      "Epoch 7/50, iter: 900/3401, mean loss: 0.0027242399425449547\n",
      "Epoch 7/50, iter: 1000/3401, mean loss: 0.46203318536895494\n",
      "Epoch 7/50, iter: 1100/3401, mean loss: 0.41842360901646314\n",
      "Epoch 7/50, iter: 1200/3401, mean loss: 0.08710207036230713\n",
      "Epoch 7/50, iter: 1300/3401, mean loss: 0.021779005056014284\n",
      "Epoch 7/50, iter: 1400/3401, mean loss: 0.022452997687651077\n",
      "Epoch 7/50, iter: 1500/3401, mean loss: 0.01224501904911449\n",
      "Epoch 7/50, iter: 1600/3401, mean loss: 0.027213604282196685\n",
      "Epoch 7/50, iter: 1700/3401, mean loss: 0.012691102396265705\n",
      "Epoch 7/50, iter: 1800/3401, mean loss: 0.6188727275026372\n",
      "Epoch 7/50, iter: 1900/3401, mean loss: 0.3422776174917817\n",
      "Epoch 7/50, iter: 2000/3401, mean loss: 0.08023425390245392\n",
      "Epoch 7/50, iter: 2100/3401, mean loss: 0.03312905794402468\n",
      "Epoch 7/50, iter: 2200/3401, mean loss: 0.031716604955727236\n",
      "Epoch 7/50, iter: 2300/3401, mean loss: 0.015021197077658144\n",
      "Epoch 7/50, iter: 2400/3401, mean loss: 0.00849452782782464\n",
      "Epoch 7/50, iter: 2500/3401, mean loss: 0.017994248322138445\n",
      "Epoch 7/50, iter: 2600/3401, mean loss: 0.6905608066182322\n",
      "Epoch 7/50, iter: 2700/3401, mean loss: 0.23658043942414225\n",
      "Epoch 7/50, iter: 2800/3401, mean loss: 0.05319767473672982\n",
      "Epoch 7/50, iter: 2900/3401, mean loss: 0.024174735258857254\n",
      "Epoch 7/50, iter: 3000/3401, mean loss: 0.020693878155507262\n",
      "Epoch 7/50, iter: 3100/3401, mean loss: 0.01903054790411261\n",
      "Epoch 7/50, iter: 3200/3401, mean loss: 0.010965085166044447\n",
      "Epoch 7/50, iter: 3300/3401, mean loss: 0.012218776836143662\n",
      "Epoch 7/50, iter: 3400/3401, mean loss: 0.007801616520891912\n",
      "Epoch 8/50, iter: 100/3401, mean loss: 0.7565247915405781\n",
      "Epoch 8/50, iter: 200/3401, mean loss: 0.047005345845827835\n",
      "Epoch 8/50, iter: 300/3401, mean loss: 0.02204086652272963\n",
      "Epoch 8/50, iter: 400/3401, mean loss: 0.009251570155174704\n",
      "Epoch 8/50, iter: 500/3401, mean loss: 0.016765383750225737\n",
      "Epoch 8/50, iter: 600/3401, mean loss: 0.009705130096754146\n",
      "Epoch 8/50, iter: 700/3401, mean loss: 0.0030860879471583755\n",
      "Epoch 8/50, iter: 800/3401, mean loss: 0.003794373096097843\n",
      "Epoch 8/50, iter: 900/3401, mean loss: 0.002307127401481921\n",
      "Epoch 8/50, iter: 1000/3401, mean loss: 0.3475987194371373\n",
      "Epoch 8/50, iter: 1100/3401, mean loss: 0.3578202400705777\n",
      "Epoch 8/50, iter: 1200/3401, mean loss: 0.0724939521253691\n",
      "Epoch 8/50, iter: 1300/3401, mean loss: 0.025316960847412702\n",
      "Epoch 8/50, iter: 1400/3401, mean loss: 0.022298673611185222\n",
      "Epoch 8/50, iter: 1500/3401, mean loss: 0.010443103814473033\n",
      "Epoch 8/50, iter: 1600/3401, mean loss: 0.030680200376418724\n",
      "Epoch 8/50, iter: 1700/3401, mean loss: 0.01619507035863535\n",
      "Epoch 8/50, iter: 1800/3401, mean loss: 0.5830130053699867\n",
      "Epoch 8/50, iter: 1900/3401, mean loss: 0.2571650916035287\n",
      "Epoch 8/50, iter: 2000/3401, mean loss: 0.07427732370124432\n",
      "Epoch 8/50, iter: 2100/3401, mean loss: 0.029155547801201463\n",
      "Epoch 8/50, iter: 2200/3401, mean loss: 0.034410326330107634\n",
      "Epoch 8/50, iter: 2300/3401, mean loss: 0.017204982676485087\n",
      "Epoch 8/50, iter: 2400/3401, mean loss: 0.010436707433163973\n",
      "Epoch 8/50, iter: 2500/3401, mean loss: 0.02250315698551276\n",
      "Epoch 8/50, iter: 2600/3401, mean loss: 0.5859394782118034\n",
      "Epoch 8/50, iter: 2700/3401, mean loss: 0.19056567825376988\n",
      "Epoch 8/50, iter: 2800/3401, mean loss: 0.049881372194504364\n",
      "Epoch 8/50, iter: 2900/3401, mean loss: 0.026515586601162794\n",
      "Epoch 8/50, iter: 3000/3401, mean loss: 0.029246992139824214\n",
      "Epoch 8/50, iter: 3100/3401, mean loss: 0.018020548152853734\n",
      "Epoch 8/50, iter: 3200/3401, mean loss: 0.02080147719373031\n",
      "Epoch 8/50, iter: 3300/3401, mean loss: 0.009731393028087041\n",
      "Epoch 8/50, iter: 3400/3401, mean loss: 0.007954708004340318\n",
      "Epoch 9/50, iter: 100/3401, mean loss: 0.6561009021685459\n",
      "Epoch 9/50, iter: 200/3401, mean loss: 0.05939960796153173\n",
      "Epoch 9/50, iter: 300/3401, mean loss: 0.02263463149742165\n",
      "Epoch 9/50, iter: 400/3401, mean loss: 0.012993086619026144\n",
      "Epoch 9/50, iter: 500/3401, mean loss: 0.022613267709148205\n",
      "Epoch 9/50, iter: 600/3401, mean loss: 0.020269790263346295\n",
      "Epoch 9/50, iter: 700/3401, mean loss: 0.0036285245253020546\n",
      "Epoch 9/50, iter: 800/3401, mean loss: 0.002754356532532256\n",
      "Epoch 9/50, iter: 900/3401, mean loss: 0.00471648032939811\n",
      "Epoch 9/50, iter: 1000/3401, mean loss: 0.33073557025386663\n",
      "Epoch 9/50, iter: 1100/3401, mean loss: 0.4304067979636602\n",
      "Epoch 9/50, iter: 1200/3401, mean loss: 0.1013048445849563\n",
      "Epoch 9/50, iter: 1300/3401, mean loss: 0.042104856673249744\n",
      "Epoch 9/50, iter: 1400/3401, mean loss: 0.023216730736858152\n",
      "Epoch 9/50, iter: 1500/3401, mean loss: 0.013365394934514824\n",
      "Epoch 9/50, iter: 1600/3401, mean loss: 0.03180626088565532\n",
      "Epoch 9/50, iter: 1700/3401, mean loss: 0.015622231276586263\n",
      "Epoch 9/50, iter: 1800/3401, mean loss: 0.5181164981169764\n",
      "Epoch 9/50, iter: 1900/3401, mean loss: 0.2369142192369327\n",
      "Epoch 9/50, iter: 2000/3401, mean loss: 0.0974761798774125\n",
      "Epoch 9/50, iter: 2100/3401, mean loss: 0.03359721367101884\n",
      "Epoch 9/50, iter: 2200/3401, mean loss: 0.03918420636820883\n",
      "Epoch 9/50, iter: 2300/3401, mean loss: 0.0249388662610545\n",
      "Epoch 9/50, iter: 2400/3401, mean loss: 0.015580930447940773\n",
      "Epoch 9/50, iter: 2500/3401, mean loss: 0.014600364740366557\n",
      "Epoch 9/50, iter: 2600/3401, mean loss: 0.6127951279895205\n",
      "Epoch 9/50, iter: 2700/3401, mean loss: 0.2359708626428619\n",
      "Epoch 9/50, iter: 2800/3401, mean loss: 0.06345539790694602\n",
      "Epoch 9/50, iter: 2900/3401, mean loss: 0.02697019646904664\n",
      "Epoch 9/50, iter: 3000/3401, mean loss: 0.025796431107573882\n",
      "Epoch 9/50, iter: 3100/3401, mean loss: 0.01589786928441754\n",
      "Epoch 9/50, iter: 3200/3401, mean loss: 0.014937731117599924\n",
      "Epoch 9/50, iter: 3300/3401, mean loss: 0.00805966504844946\n",
      "Epoch 9/50, iter: 3400/3401, mean loss: 0.014934190459114234\n",
      "Epoch 10/50, iter: 100/3401, mean loss: 0.5661771441018209\n",
      "Epoch 10/50, iter: 200/3401, mean loss: 0.05381118593264546\n",
      "Epoch 10/50, iter: 300/3401, mean loss: 0.024380418961227404\n",
      "Epoch 10/50, iter: 400/3401, mean loss: 0.013738696439177148\n",
      "Epoch 10/50, iter: 500/3401, mean loss: 0.02013777689950075\n",
      "Epoch 10/50, iter: 600/3401, mean loss: 0.016779561767314703\n",
      "Epoch 10/50, iter: 700/3401, mean loss: 0.0025678646023516195\n",
      "Epoch 10/50, iter: 800/3401, mean loss: 0.004566687051024019\n",
      "Epoch 10/50, iter: 900/3401, mean loss: 0.015335353490879697\n",
      "Epoch 10/50, iter: 1000/3401, mean loss: 0.4191372616336071\n",
      "Epoch 10/50, iter: 1100/3401, mean loss: 0.33922235341509804\n",
      "Epoch 10/50, iter: 1200/3401, mean loss: 0.11079963369997131\n",
      "Epoch 10/50, iter: 1300/3401, mean loss: 0.04904376415688603\n",
      "Epoch 10/50, iter: 1400/3401, mean loss: 0.027262065172089935\n",
      "Epoch 10/50, iter: 1500/3401, mean loss: 0.01943440663610545\n",
      "Epoch 10/50, iter: 1600/3401, mean loss: 0.027157410557105095\n",
      "Epoch 10/50, iter: 1700/3401, mean loss: 0.019836702613237094\n",
      "Epoch 10/50, iter: 1800/3401, mean loss: 0.4020984924513584\n",
      "Epoch 10/50, iter: 1900/3401, mean loss: 0.22538528600067365\n",
      "Epoch 10/50, iter: 2000/3401, mean loss: 0.09914573273461429\n",
      "Epoch 10/50, iter: 2100/3401, mean loss: 0.04283786492829677\n",
      "Epoch 10/50, iter: 2200/3401, mean loss: 0.035243412809968504\n",
      "Epoch 10/50, iter: 2300/3401, mean loss: 0.02178575201712192\n",
      "Epoch 10/50, iter: 2400/3401, mean loss: 0.02730928469899709\n",
      "Epoch 10/50, iter: 2500/3401, mean loss: 0.016627780008864192\n",
      "Epoch 10/50, iter: 2600/3401, mean loss: 0.6370552853528716\n",
      "Epoch 10/50, iter: 2700/3401, mean loss: 0.21594812319322954\n",
      "Epoch 10/50, iter: 2800/3401, mean loss: 0.06025777260947507\n",
      "Epoch 10/50, iter: 2900/3401, mean loss: 0.026832230081490705\n",
      "Epoch 10/50, iter: 3000/3401, mean loss: 0.023943379942847967\n",
      "Epoch 10/50, iter: 3100/3401, mean loss: 0.019867248485202254\n",
      "Epoch 10/50, iter: 3200/3401, mean loss: 0.01481828031633995\n",
      "Epoch 10/50, iter: 3300/3401, mean loss: 0.006637084009089449\n",
      "Epoch 10/50, iter: 3400/3401, mean loss: 0.007659202986058631\n",
      "Epoch 11/50, iter: 100/3401, mean loss: 0.5805268524645362\n",
      "Epoch 11/50, iter: 200/3401, mean loss: 0.07451415185903898\n",
      "Epoch 11/50, iter: 300/3401, mean loss: 0.041191725821845465\n",
      "Epoch 11/50, iter: 400/3401, mean loss: 0.013885641722945366\n",
      "Epoch 11/50, iter: 500/3401, mean loss: 0.029371964317615493\n",
      "Epoch 11/50, iter: 600/3401, mean loss: 0.019492943969994486\n",
      "Epoch 11/50, iter: 700/3401, mean loss: 0.004549032036547942\n",
      "Epoch 11/50, iter: 800/3401, mean loss: 0.003912714286555001\n",
      "Epoch 11/50, iter: 900/3401, mean loss: 0.0054182059542836215\n",
      "Epoch 11/50, iter: 1000/3401, mean loss: 0.29081535707677175\n",
      "Epoch 11/50, iter: 1100/3401, mean loss: 0.3624218529294012\n",
      "Epoch 11/50, iter: 1200/3401, mean loss: 0.1309955297873239\n",
      "Epoch 11/50, iter: 1300/3401, mean loss: 0.04209712980443556\n",
      "Epoch 11/50, iter: 1400/3401, mean loss: 0.03816397582110767\n",
      "Epoch 11/50, iter: 1500/3401, mean loss: 0.016648900709294593\n",
      "Epoch 11/50, iter: 1600/3401, mean loss: 0.03569273660861882\n",
      "Epoch 11/50, iter: 1700/3401, mean loss: 0.011588711326840126\n",
      "Epoch 11/50, iter: 1800/3401, mean loss: 0.3336195898441272\n",
      "Epoch 11/50, iter: 1900/3401, mean loss: 0.2774400588369463\n",
      "Epoch 11/50, iter: 2000/3401, mean loss: 0.09019423621226451\n",
      "Epoch 11/50, iter: 2100/3401, mean loss: 0.03446984564216109\n",
      "Epoch 11/50, iter: 2200/3401, mean loss: 0.038393791301532476\n",
      "Epoch 11/50, iter: 2300/3401, mean loss: 0.018819164089582046\n",
      "Epoch 11/50, iter: 2400/3401, mean loss: 0.02035077450229437\n",
      "Epoch 11/50, iter: 2500/3401, mean loss: 0.014785978374753839\n",
      "Epoch 11/50, iter: 2600/3401, mean loss: 0.5874295569940022\n",
      "Epoch 11/50, iter: 2700/3401, mean loss: 0.18257982862261998\n",
      "Epoch 11/50, iter: 2800/3401, mean loss: 0.0397633502818644\n",
      "Epoch 11/50, iter: 2900/3401, mean loss: 0.02340062805720663\n",
      "Epoch 11/50, iter: 3000/3401, mean loss: 0.020343767694885172\n",
      "Epoch 11/50, iter: 3100/3401, mean loss: 0.015564929740863818\n",
      "Epoch 11/50, iter: 3200/3401, mean loss: 0.01904271450057081\n",
      "Epoch 11/50, iter: 3300/3401, mean loss: 0.007993530355118423\n",
      "Epoch 11/50, iter: 3400/3401, mean loss: 0.004431820708712167\n",
      "Epoch 12/50, iter: 100/3401, mean loss: 0.43271263906237434\n",
      "Epoch 12/50, iter: 200/3401, mean loss: 0.04378430681988903\n",
      "Epoch 12/50, iter: 300/3401, mean loss: 0.030718620462903346\n",
      "Epoch 12/50, iter: 400/3401, mean loss: 0.011682713013724423\n",
      "Epoch 12/50, iter: 500/3401, mean loss: 0.02840675418592582\n",
      "Epoch 12/50, iter: 600/3401, mean loss: 0.03224075260014615\n",
      "Epoch 12/50, iter: 700/3401, mean loss: 0.0030151579156745354\n",
      "Epoch 12/50, iter: 800/3401, mean loss: 0.0033994673191477887\n",
      "Epoch 12/50, iter: 900/3401, mean loss: 0.004335497842916425\n",
      "Epoch 12/50, iter: 1000/3401, mean loss: 0.31851390704720245\n",
      "Epoch 12/50, iter: 1100/3401, mean loss: 0.24002012524026214\n",
      "Epoch 12/50, iter: 1200/3401, mean loss: 0.09709165411955838\n",
      "Epoch 12/50, iter: 1300/3401, mean loss: 0.027433606113954737\n",
      "Epoch 12/50, iter: 1400/3401, mean loss: 0.015852265300923138\n",
      "Epoch 12/50, iter: 1500/3401, mean loss: 0.015967680979952378\n",
      "Epoch 12/50, iter: 1600/3401, mean loss: 0.03933618973520936\n",
      "Epoch 12/50, iter: 1700/3401, mean loss: 0.011223986058437276\n",
      "Epoch 12/50, iter: 1800/3401, mean loss: 0.2737686880108869\n",
      "Epoch 12/50, iter: 1900/3401, mean loss: 0.20045248238544444\n",
      "Epoch 12/50, iter: 2000/3401, mean loss: 0.07427458835507422\n",
      "Epoch 12/50, iter: 2100/3401, mean loss: 0.034057618657097916\n",
      "Epoch 12/50, iter: 2200/3401, mean loss: 0.04146366424181906\n",
      "Epoch 12/50, iter: 2300/3401, mean loss: 0.03620489330389318\n",
      "Epoch 12/50, iter: 2400/3401, mean loss: 0.011352166182596192\n",
      "Epoch 12/50, iter: 2500/3401, mean loss: 0.0238863678402808\n",
      "Epoch 12/50, iter: 2600/3401, mean loss: 0.587912536754323\n",
      "Epoch 12/50, iter: 2700/3401, mean loss: 0.1749477423063945\n",
      "Epoch 12/50, iter: 2800/3401, mean loss: 0.050540756637783485\n",
      "Epoch 12/50, iter: 2900/3401, mean loss: 0.024856223009501265\n",
      "Epoch 12/50, iter: 3000/3401, mean loss: 0.016916634691297075\n",
      "Epoch 12/50, iter: 3100/3401, mean loss: 0.045757627084392426\n",
      "Epoch 12/50, iter: 3200/3401, mean loss: 0.010676152201415618\n",
      "Epoch 12/50, iter: 3300/3401, mean loss: 0.008976721919761203\n",
      "Epoch 12/50, iter: 3400/3401, mean loss: 0.010497654832776674\n",
      "Epoch 13/50, iter: 100/3401, mean loss: 0.33699081777143874\n",
      "Epoch 13/50, iter: 200/3401, mean loss: 0.03333674016024815\n",
      "Epoch 13/50, iter: 300/3401, mean loss: 0.02637026863203573\n",
      "Epoch 13/50, iter: 400/3401, mean loss: 0.0171478372816091\n",
      "Epoch 13/50, iter: 500/3401, mean loss: 0.032859060610126105\n",
      "Epoch 13/50, iter: 600/3401, mean loss: 0.01710448735153477\n",
      "Epoch 13/50, iter: 700/3401, mean loss: 0.004972948439281026\n",
      "Epoch 13/50, iter: 800/3401, mean loss: 0.0036528899765608004\n",
      "Epoch 13/50, iter: 900/3401, mean loss: 0.0033578108752089974\n",
      "Epoch 13/50, iter: 1000/3401, mean loss: 0.18407174640681917\n",
      "Epoch 13/50, iter: 1100/3401, mean loss: 0.22264128183218418\n",
      "Epoch 13/50, iter: 1200/3401, mean loss: 0.110839533388571\n",
      "Epoch 13/50, iter: 1300/3401, mean loss: 0.030658440844561027\n",
      "Epoch 13/50, iter: 1400/3401, mean loss: 0.01688737599519186\n",
      "Epoch 13/50, iter: 1500/3401, mean loss: 0.011345511796703818\n",
      "Epoch 13/50, iter: 1600/3401, mean loss: 0.030960740672609857\n",
      "Epoch 13/50, iter: 1700/3401, mean loss: 0.016061032034387496\n",
      "Epoch 13/50, iter: 1800/3401, mean loss: 0.2925222624940066\n",
      "Epoch 13/50, iter: 1900/3401, mean loss: 0.22483996716764523\n",
      "Epoch 13/50, iter: 2000/3401, mean loss: 0.10139721726751304\n",
      "Epoch 13/50, iter: 2100/3401, mean loss: 0.045646847871039425\n",
      "Epoch 13/50, iter: 2200/3401, mean loss: 0.027956008474880036\n",
      "Epoch 13/50, iter: 2300/3401, mean loss: 0.03185330093750054\n",
      "Epoch 13/50, iter: 2400/3401, mean loss: 0.014686042312969789\n",
      "Epoch 13/50, iter: 2500/3401, mean loss: 0.02801782718576078\n",
      "Epoch 13/50, iter: 2600/3401, mean loss: 0.6238591457743405\n",
      "Epoch 13/50, iter: 2700/3401, mean loss: 0.16623392428591616\n",
      "Epoch 13/50, iter: 2800/3401, mean loss: 0.07850152612169041\n",
      "Epoch 13/50, iter: 2900/3401, mean loss: 0.044541592412679167\n",
      "Epoch 13/50, iter: 3000/3401, mean loss: 0.03171979146750346\n",
      "Epoch 13/50, iter: 3100/3401, mean loss: 0.023087549341789783\n",
      "Epoch 13/50, iter: 3200/3401, mean loss: 0.011199488378712772\n",
      "Epoch 13/50, iter: 3300/3401, mean loss: 0.009508421533458886\n",
      "Epoch 13/50, iter: 3400/3401, mean loss: 0.012351302805394298\n",
      "Epoch 14/50, iter: 100/3401, mean loss: 0.39582490444736323\n",
      "Epoch 14/50, iter: 200/3401, mean loss: 0.04521225573629636\n",
      "Epoch 14/50, iter: 300/3401, mean loss: 0.029091331437102783\n",
      "Epoch 14/50, iter: 400/3401, mean loss: 0.007443492112283821\n",
      "Epoch 14/50, iter: 500/3401, mean loss: 0.04934654026104681\n",
      "Epoch 14/50, iter: 600/3401, mean loss: 0.03139139155898192\n",
      "Epoch 14/50, iter: 700/3401, mean loss: 0.003614435160445737\n",
      "Epoch 14/50, iter: 800/3401, mean loss: 0.003403954061650438\n",
      "Epoch 14/50, iter: 900/3401, mean loss: 0.02965738147003151\n",
      "Epoch 14/50, iter: 1000/3401, mean loss: 0.19182370755873407\n",
      "Epoch 14/50, iter: 1100/3401, mean loss: 0.21367422305076617\n",
      "Epoch 14/50, iter: 1200/3401, mean loss: 0.08934236076729576\n",
      "Epoch 14/50, iter: 1300/3401, mean loss: 0.04604245068803039\n",
      "Epoch 14/50, iter: 1400/3401, mean loss: 0.033515647360372894\n",
      "Epoch 14/50, iter: 1500/3401, mean loss: 0.02593035010937797\n",
      "Epoch 14/50, iter: 1600/3401, mean loss: 0.025318266589490292\n",
      "Epoch 14/50, iter: 1700/3401, mean loss: 0.011589643519176178\n",
      "Epoch 14/50, iter: 1800/3401, mean loss: 0.40887966578639406\n",
      "Epoch 14/50, iter: 1900/3401, mean loss: 0.19897081817211074\n",
      "Epoch 14/50, iter: 2000/3401, mean loss: 0.08835684982253951\n",
      "Epoch 14/50, iter: 2100/3401, mean loss: 0.033334233447885706\n",
      "Epoch 14/50, iter: 2200/3401, mean loss: 0.03453312489843483\n",
      "Epoch 14/50, iter: 2300/3401, mean loss: 0.029157874528964386\n",
      "Epoch 14/50, iter: 2400/3401, mean loss: 0.028114775031369846\n",
      "Epoch 14/50, iter: 2500/3401, mean loss: 0.029164166998162953\n",
      "Epoch 14/50, iter: 2600/3401, mean loss: 0.4984659096446086\n",
      "Epoch 14/50, iter: 2700/3401, mean loss: 0.1562138187151868\n",
      "Epoch 14/50, iter: 2800/3401, mean loss: 0.06473779745287174\n",
      "Epoch 14/50, iter: 2900/3401, mean loss: 0.028991165259403717\n",
      "Epoch 14/50, iter: 3000/3401, mean loss: 0.020022757527461865\n",
      "Epoch 14/50, iter: 3100/3401, mean loss: 0.026432730605679494\n",
      "Epoch 14/50, iter: 3200/3401, mean loss: 0.008669417325812105\n",
      "Epoch 14/50, iter: 3300/3401, mean loss: 0.006623594954822956\n",
      "Epoch 14/50, iter: 3400/3401, mean loss: 0.009222013426158924\n",
      "Epoch 15/50, iter: 100/3401, mean loss: 0.26509091076630287\n",
      "Epoch 15/50, iter: 200/3401, mean loss: 0.04741470041877619\n",
      "Epoch 15/50, iter: 300/3401, mean loss: 0.023436207414590626\n",
      "Epoch 15/50, iter: 400/3401, mean loss: 0.007408220674176391\n",
      "Epoch 15/50, iter: 500/3401, mean loss: 0.03518286013820557\n",
      "Epoch 15/50, iter: 600/3401, mean loss: 0.023765684039953782\n",
      "Epoch 15/50, iter: 700/3401, mean loss: 0.004775656797347665\n",
      "Epoch 15/50, iter: 800/3401, mean loss: 0.002357246449143986\n",
      "Epoch 15/50, iter: 900/3401, mean loss: 0.02474124697095533\n",
      "Epoch 15/50, iter: 1000/3401, mean loss: 0.23169590145902305\n",
      "Epoch 15/50, iter: 1100/3401, mean loss: 0.21522650053635517\n",
      "Epoch 15/50, iter: 1200/3401, mean loss: 0.0730792024182756\n",
      "Epoch 15/50, iter: 1300/3401, mean loss: 0.028306673568295082\n",
      "Epoch 15/50, iter: 1400/3401, mean loss: 0.025375334142609063\n",
      "Epoch 15/50, iter: 1500/3401, mean loss: 0.025056538704966443\n",
      "Epoch 15/50, iter: 1600/3401, mean loss: 0.02972474958394006\n",
      "Epoch 15/50, iter: 1700/3401, mean loss: 0.019096070250614616\n",
      "Epoch 15/50, iter: 1800/3401, mean loss: 0.2706862642383226\n",
      "Epoch 15/50, iter: 1900/3401, mean loss: 0.21870851592873805\n",
      "Epoch 15/50, iter: 2000/3401, mean loss: 0.09189810065221536\n",
      "Epoch 15/50, iter: 2100/3401, mean loss: 0.03193936047384341\n",
      "Epoch 15/50, iter: 2200/3401, mean loss: 0.04348696836002091\n",
      "Epoch 15/50, iter: 2300/3401, mean loss: 0.02852861243220701\n",
      "Epoch 15/50, iter: 2400/3401, mean loss: 0.02312215571145231\n",
      "Epoch 15/50, iter: 2500/3401, mean loss: 0.023872506750859942\n",
      "Epoch 15/50, iter: 2600/3401, mean loss: 0.5158542716911774\n",
      "Epoch 15/50, iter: 2700/3401, mean loss: 0.10311548810663225\n",
      "Epoch 15/50, iter: 2800/3401, mean loss: 0.04774162436377082\n",
      "Epoch 15/50, iter: 2900/3401, mean loss: 0.03717766847801613\n",
      "Epoch 15/50, iter: 3000/3401, mean loss: 0.03238886778287906\n",
      "Epoch 15/50, iter: 3100/3401, mean loss: 0.030003660341583326\n",
      "Epoch 15/50, iter: 3200/3401, mean loss: 0.013502827296196784\n",
      "Epoch 15/50, iter: 3300/3401, mean loss: 0.011513062927657388\n",
      "Epoch 15/50, iter: 3400/3401, mean loss: 0.008906567319844499\n",
      "Epoch 16/50, iter: 100/3401, mean loss: 0.24795308136890526\n",
      "Epoch 16/50, iter: 200/3401, mean loss: 0.04991459412265613\n",
      "Epoch 16/50, iter: 300/3401, mean loss: 0.01588280329990084\n",
      "Epoch 16/50, iter: 400/3401, mean loss: 0.01064260168564715\n",
      "Epoch 16/50, iter: 500/3401, mean loss: 0.02027740427221488\n",
      "Epoch 16/50, iter: 600/3401, mean loss: 0.028316113830326374\n",
      "Epoch 16/50, iter: 700/3401, mean loss: 0.004253548679025698\n",
      "Epoch 16/50, iter: 800/3401, mean loss: 0.0029854505624555827\n",
      "Epoch 16/50, iter: 900/3401, mean loss: 0.014096697650983288\n",
      "Epoch 16/50, iter: 1000/3401, mean loss: 0.18186467584117622\n",
      "Epoch 16/50, iter: 1100/3401, mean loss: 0.2712248934647141\n",
      "Epoch 16/50, iter: 1200/3401, mean loss: 0.10494986619129464\n",
      "Epoch 16/50, iter: 1300/3401, mean loss: 0.032438588562590666\n",
      "Epoch 16/50, iter: 1400/3401, mean loss: 0.02514374198701944\n",
      "Epoch 16/50, iter: 1500/3401, mean loss: 0.01701836873655793\n",
      "Epoch 16/50, iter: 1600/3401, mean loss: 0.04440682285200097\n",
      "Epoch 16/50, iter: 1700/3401, mean loss: 0.019386234897820244\n",
      "Epoch 16/50, iter: 1800/3401, mean loss: 0.2188283609574819\n",
      "Epoch 16/50, iter: 1900/3401, mean loss: 0.1848785457998747\n",
      "Epoch 16/50, iter: 2000/3401, mean loss: 0.1035399380636295\n",
      "Epoch 16/50, iter: 2100/3401, mean loss: 0.02464273890606819\n",
      "Epoch 16/50, iter: 2200/3401, mean loss: 0.029664123322381785\n",
      "Epoch 16/50, iter: 2300/3401, mean loss: 0.017115162627905533\n",
      "Epoch 16/50, iter: 2400/3401, mean loss: 0.015014266727105366\n",
      "Epoch 16/50, iter: 2500/3401, mean loss: 0.012974376338412412\n",
      "Epoch 16/50, iter: 2600/3401, mean loss: 0.4341016498461611\n",
      "Epoch 16/50, iter: 2700/3401, mean loss: 0.15314337906733272\n",
      "Epoch 16/50, iter: 2800/3401, mean loss: 0.05190232171633397\n",
      "Epoch 16/50, iter: 2900/3401, mean loss: 0.022428062428789558\n",
      "Epoch 16/50, iter: 3000/3401, mean loss: 0.015397707068185653\n",
      "Epoch 16/50, iter: 3100/3401, mean loss: 0.018895218842388744\n",
      "Epoch 16/50, iter: 3200/3401, mean loss: 0.015925756882405436\n",
      "Epoch 16/50, iter: 3300/3401, mean loss: 0.006328327275537227\n",
      "Epoch 16/50, iter: 3400/3401, mean loss: 0.006473752001768176\n",
      "Epoch 17/50, iter: 100/3401, mean loss: 0.20155146974459057\n",
      "Epoch 17/50, iter: 200/3401, mean loss: 0.0682071692192585\n",
      "Epoch 17/50, iter: 300/3401, mean loss: 0.026689136664536477\n",
      "Epoch 17/50, iter: 400/3401, mean loss: 0.007700521002791447\n",
      "Epoch 17/50, iter: 500/3401, mean loss: 0.03330982842484481\n",
      "Epoch 17/50, iter: 600/3401, mean loss: 0.027132146862905417\n",
      "Epoch 17/50, iter: 700/3401, mean loss: 0.006948825934241966\n",
      "Epoch 17/50, iter: 800/3401, mean loss: 0.0014753181706359442\n",
      "Epoch 17/50, iter: 900/3401, mean loss: 0.010850340076358407\n",
      "Epoch 17/50, iter: 1000/3401, mean loss: 0.15802819816611036\n",
      "Epoch 17/50, iter: 1100/3401, mean loss: 0.2225332868638725\n",
      "Epoch 17/50, iter: 1200/3401, mean loss: 0.10541394551623569\n",
      "Epoch 17/50, iter: 1300/3401, mean loss: 0.04920535358651506\n",
      "Epoch 17/50, iter: 1400/3401, mean loss: 0.022643059443777817\n",
      "Epoch 17/50, iter: 1500/3401, mean loss: 0.012373656682426883\n",
      "Epoch 17/50, iter: 1600/3401, mean loss: 0.04043700419607376\n",
      "Epoch 17/50, iter: 1700/3401, mean loss: 0.009905973026417669\n",
      "Epoch 17/50, iter: 1800/3401, mean loss: 0.2576572198224689\n",
      "Epoch 17/50, iter: 1900/3401, mean loss: 0.18076940483799261\n",
      "Epoch 17/50, iter: 2000/3401, mean loss: 0.09204020923538224\n",
      "Epoch 17/50, iter: 2100/3401, mean loss: 0.045512913799568795\n",
      "Epoch 17/50, iter: 2200/3401, mean loss: 0.029983854368562105\n",
      "Epoch 17/50, iter: 2300/3401, mean loss: 0.015409171116517087\n",
      "Epoch 17/50, iter: 2400/3401, mean loss: 0.027357465900587387\n",
      "Epoch 17/50, iter: 2500/3401, mean loss: 0.02136168289033918\n",
      "Epoch 17/50, iter: 2600/3401, mean loss: 0.45625681348382613\n",
      "Epoch 17/50, iter: 2700/3401, mean loss: 0.1763461681700119\n",
      "Epoch 17/50, iter: 2800/3401, mean loss: 0.05594363484247879\n",
      "Epoch 17/50, iter: 2900/3401, mean loss: 0.025287110148914282\n",
      "Epoch 17/50, iter: 3000/3401, mean loss: 0.01796055773229682\n",
      "Epoch 17/50, iter: 3100/3401, mean loss: 0.018951544907926773\n",
      "Epoch 17/50, iter: 3200/3401, mean loss: 0.012599766315080564\n",
      "Epoch 17/50, iter: 3300/3401, mean loss: 0.00872899050817857\n",
      "Epoch 17/50, iter: 3400/3401, mean loss: 0.007506994803291462\n",
      "Epoch 18/50, iter: 100/3401, mean loss: 0.1710151925606533\n",
      "Epoch 18/50, iter: 200/3401, mean loss: 0.041297913129501464\n",
      "Epoch 18/50, iter: 300/3401, mean loss: 0.03650398490639191\n",
      "Epoch 18/50, iter: 400/3401, mean loss: 0.010040539658551211\n",
      "Epoch 18/50, iter: 500/3401, mean loss: 0.026197341955967773\n",
      "Epoch 18/50, iter: 600/3401, mean loss: 0.036360977434322025\n",
      "Epoch 18/50, iter: 700/3401, mean loss: 0.0019335601456555906\n",
      "Epoch 18/50, iter: 800/3401, mean loss: 0.0028786279221800016\n",
      "Epoch 18/50, iter: 900/3401, mean loss: 0.0029418636253748785\n",
      "Epoch 18/50, iter: 1000/3401, mean loss: 0.12061639581447654\n",
      "Epoch 18/50, iter: 1100/3401, mean loss: 0.18569793832117285\n",
      "Epoch 18/50, iter: 1200/3401, mean loss: 0.10035720947313635\n",
      "Epoch 18/50, iter: 1300/3401, mean loss: 0.01864681747698995\n",
      "Epoch 18/50, iter: 1400/3401, mean loss: 0.025337746862708743\n",
      "Epoch 18/50, iter: 1500/3401, mean loss: 0.013564699385606076\n",
      "Epoch 18/50, iter: 1600/3401, mean loss: 0.02141850653851499\n",
      "Epoch 18/50, iter: 1700/3401, mean loss: 0.019884672760539956\n",
      "Epoch 18/50, iter: 1800/3401, mean loss: 0.29446671073726405\n",
      "Epoch 18/50, iter: 1900/3401, mean loss: 0.17692694111250604\n",
      "Epoch 18/50, iter: 2000/3401, mean loss: 0.09205125394411424\n",
      "Epoch 18/50, iter: 2100/3401, mean loss: 0.05221980324891547\n",
      "Epoch 18/50, iter: 2200/3401, mean loss: 0.02946358700765984\n",
      "Epoch 18/50, iter: 2300/3401, mean loss: 0.041754298326937944\n",
      "Epoch 18/50, iter: 2400/3401, mean loss: 0.012646705747748115\n",
      "Epoch 18/50, iter: 2500/3401, mean loss: 0.01660710941101968\n",
      "Epoch 18/50, iter: 2600/3401, mean loss: 0.5007283490765809\n",
      "Epoch 18/50, iter: 2700/3401, mean loss: 0.2012359159824041\n",
      "Epoch 18/50, iter: 2800/3401, mean loss: 0.048829734343134985\n",
      "Epoch 18/50, iter: 2900/3401, mean loss: 0.026332593641532186\n",
      "Epoch 18/50, iter: 3000/3401, mean loss: 0.019048391918153128\n",
      "Epoch 18/50, iter: 3100/3401, mean loss: 0.027798531044380966\n",
      "Epoch 18/50, iter: 3200/3401, mean loss: 0.031209018002589028\n",
      "Epoch 18/50, iter: 3300/3401, mean loss: 0.03158291823315142\n",
      "Epoch 18/50, iter: 3400/3401, mean loss: 0.02883615425087722\n",
      "Epoch 19/50, iter: 100/3401, mean loss: 0.20248670677057817\n",
      "Epoch 19/50, iter: 200/3401, mean loss: 0.05719108929792128\n",
      "Epoch 19/50, iter: 300/3401, mean loss: 0.027251648039369344\n",
      "Epoch 19/50, iter: 400/3401, mean loss: 0.0166341721842133\n",
      "Epoch 19/50, iter: 500/3401, mean loss: 0.03801955119285367\n",
      "Epoch 19/50, iter: 600/3401, mean loss: 0.026603615142968308\n",
      "Epoch 19/50, iter: 700/3401, mean loss: 0.008281424486191895\n",
      "Epoch 19/50, iter: 800/3401, mean loss: 0.003068661367307506\n",
      "Epoch 19/50, iter: 900/3401, mean loss: 0.015242860738779597\n",
      "Epoch 19/50, iter: 1000/3401, mean loss: 0.15626260013298862\n",
      "Epoch 19/50, iter: 1100/3401, mean loss: 0.2046663967942368\n",
      "Epoch 19/50, iter: 1200/3401, mean loss: 0.14042792792795808\n",
      "Epoch 19/50, iter: 1300/3401, mean loss: 0.028067775706572037\n",
      "Epoch 19/50, iter: 1400/3401, mean loss: 0.025645370429218134\n",
      "Epoch 19/50, iter: 1500/3401, mean loss: 0.016662821552504283\n",
      "Epoch 19/50, iter: 1600/3401, mean loss: 0.020826945495992958\n",
      "Epoch 19/50, iter: 1700/3401, mean loss: 0.007691760200979907\n",
      "Epoch 19/50, iter: 1800/3401, mean loss: 0.2859402885465629\n",
      "Epoch 19/50, iter: 1900/3401, mean loss: 0.2511949057050515\n",
      "Epoch 19/50, iter: 2000/3401, mean loss: 0.10274895729126002\n",
      "Epoch 19/50, iter: 2100/3401, mean loss: 0.05169694260025096\n",
      "Epoch 19/50, iter: 2200/3401, mean loss: 0.04528917409743599\n",
      "Epoch 19/50, iter: 2300/3401, mean loss: 0.027865102833896458\n",
      "Epoch 19/50, iter: 2400/3401, mean loss: 0.01899858623633122\n",
      "Epoch 19/50, iter: 2500/3401, mean loss: 0.02164253586372787\n",
      "Epoch 19/50, iter: 2600/3401, mean loss: 0.4445673847762114\n",
      "Epoch 19/50, iter: 2700/3401, mean loss: 0.14518879486055083\n",
      "Epoch 19/50, iter: 2800/3401, mean loss: 0.05794415289500136\n",
      "Epoch 19/50, iter: 2900/3401, mean loss: 0.031265550476273635\n",
      "Epoch 19/50, iter: 3000/3401, mean loss: 0.020680240437037298\n",
      "Epoch 19/50, iter: 3100/3401, mean loss: 0.01865088248106872\n",
      "Epoch 19/50, iter: 3200/3401, mean loss: 0.03269930463228654\n",
      "Epoch 19/50, iter: 3300/3401, mean loss: 0.017774487406909536\n",
      "Epoch 19/50, iter: 3400/3401, mean loss: 0.012952024997768881\n",
      "Epoch 20/50, iter: 100/3401, mean loss: 0.20635252348805352\n",
      "Epoch 20/50, iter: 200/3401, mean loss: 0.05591686063993507\n",
      "Epoch 20/50, iter: 300/3401, mean loss: 0.017349949767892666\n",
      "Epoch 20/50, iter: 400/3401, mean loss: 0.013640688189713045\n",
      "Epoch 20/50, iter: 500/3401, mean loss: 0.038677445647349484\n",
      "Epoch 20/50, iter: 600/3401, mean loss: 0.02346886295664831\n",
      "Epoch 20/50, iter: 700/3401, mean loss: 0.004149317562461263\n",
      "Epoch 20/50, iter: 800/3401, mean loss: 0.004206636535735697\n",
      "Epoch 20/50, iter: 900/3401, mean loss: 0.005584305143152335\n",
      "Epoch 20/50, iter: 1000/3401, mean loss: 0.13178627349935085\n",
      "Epoch 20/50, iter: 1100/3401, mean loss: 0.20546609279391304\n",
      "Epoch 20/50, iter: 1200/3401, mean loss: 0.11640683686351792\n",
      "Epoch 20/50, iter: 1300/3401, mean loss: 0.05066194261412654\n",
      "Epoch 20/50, iter: 1400/3401, mean loss: 0.03116847460717736\n",
      "Epoch 20/50, iter: 1500/3401, mean loss: 0.03535236764598615\n",
      "Epoch 20/50, iter: 1600/3401, mean loss: 0.0486041399654858\n",
      "Epoch 20/50, iter: 1700/3401, mean loss: 0.01702468259014168\n",
      "Epoch 20/50, iter: 1800/3401, mean loss: 0.37584791127969325\n",
      "Epoch 20/50, iter: 1900/3401, mean loss: 0.19929769882084655\n",
      "Epoch 20/50, iter: 2000/3401, mean loss: 0.1332398825160817\n",
      "Epoch 20/50, iter: 2100/3401, mean loss: 0.05815114537686441\n",
      "Epoch 20/50, iter: 2200/3401, mean loss: 0.04436691288671\n",
      "Epoch 20/50, iter: 2300/3401, mean loss: 0.025583298943350884\n",
      "Epoch 20/50, iter: 2400/3401, mean loss: 0.0251770808308747\n",
      "Epoch 20/50, iter: 2500/3401, mean loss: 0.02537639282274803\n",
      "Epoch 20/50, iter: 2600/3401, mean loss: 0.44308792412867715\n",
      "Epoch 20/50, iter: 2700/3401, mean loss: 0.20269765244112933\n",
      "Epoch 20/50, iter: 2800/3401, mean loss: 0.04923955585681142\n",
      "Epoch 20/50, iter: 2900/3401, mean loss: 0.031338332500243385\n",
      "Epoch 20/50, iter: 3000/3401, mean loss: 0.029936798189438604\n",
      "Epoch 20/50, iter: 3100/3401, mean loss: 0.026248499819157906\n",
      "Epoch 20/50, iter: 3200/3401, mean loss: 0.05366121899267281\n",
      "Epoch 20/50, iter: 3300/3401, mean loss: 0.011165894885828642\n",
      "Epoch 20/50, iter: 3400/3401, mean loss: 0.026108415079475193\n",
      "Epoch 21/50, iter: 100/3401, mean loss: 0.18799327766793794\n",
      "Epoch 21/50, iter: 200/3401, mean loss: 0.056277394803452126\n",
      "Epoch 21/50, iter: 300/3401, mean loss: 0.021841851365028333\n",
      "Epoch 21/50, iter: 400/3401, mean loss: 0.02758871093682046\n",
      "Epoch 21/50, iter: 500/3401, mean loss: 0.04933337388157184\n",
      "Epoch 21/50, iter: 600/3401, mean loss: 0.0381515180928114\n",
      "Epoch 21/50, iter: 700/3401, mean loss: 0.008068491639188552\n",
      "Epoch 21/50, iter: 800/3401, mean loss: 0.0036230025615274285\n",
      "Epoch 21/50, iter: 900/3401, mean loss: 0.006735176777114092\n",
      "Epoch 21/50, iter: 1000/3401, mean loss: 0.19068994136485828\n",
      "Epoch 21/50, iter: 1100/3401, mean loss: 0.35728691022735803\n",
      "Epoch 21/50, iter: 1200/3401, mean loss: 0.137414454804632\n",
      "Epoch 21/50, iter: 1300/3401, mean loss: 0.06701319903648141\n",
      "Epoch 21/50, iter: 1400/3401, mean loss: 0.029996871873780435\n",
      "Epoch 21/50, iter: 1500/3401, mean loss: 0.031505039805220465\n",
      "Epoch 21/50, iter: 1600/3401, mean loss: 0.05475207116710308\n",
      "Epoch 21/50, iter: 1700/3401, mean loss: 0.02021518406704402\n",
      "Epoch 21/50, iter: 1800/3401, mean loss: 0.25626751845533247\n",
      "Epoch 21/50, iter: 1900/3401, mean loss: 0.3722973763521077\n",
      "Epoch 21/50, iter: 2000/3401, mean loss: 0.14012360159562376\n",
      "Epoch 21/50, iter: 2100/3401, mean loss: 0.06355794953986106\n",
      "Epoch 21/50, iter: 2200/3401, mean loss: 0.05958171694639305\n",
      "Epoch 21/50, iter: 2300/3401, mean loss: 0.04576662047976498\n",
      "Epoch 21/50, iter: 2400/3401, mean loss: 0.030875547871946765\n",
      "Epoch 21/50, iter: 2500/3401, mean loss: 0.02975682758241362\n",
      "Epoch 21/50, iter: 2600/3401, mean loss: 0.5446547498845155\n",
      "Epoch 21/50, iter: 2700/3401, mean loss: 0.18194344639312618\n",
      "Epoch 21/50, iter: 2800/3401, mean loss: 0.06253445082109636\n",
      "Epoch 21/50, iter: 2900/3401, mean loss: 0.028632402264863684\n",
      "Epoch 21/50, iter: 3000/3401, mean loss: 0.05970372680090364\n",
      "Epoch 21/50, iter: 3100/3401, mean loss: 0.020370079558833964\n",
      "Epoch 21/50, iter: 3200/3401, mean loss: 0.027380841983583402\n",
      "Epoch 21/50, iter: 3300/3401, mean loss: 0.007454124542511842\n",
      "Epoch 21/50, iter: 3400/3401, mean loss: 0.0439995773766292\n",
      "Epoch 22/50, iter: 100/3401, mean loss: 0.25909805502946254\n",
      "Epoch 22/50, iter: 200/3401, mean loss: 0.06657982555671879\n",
      "Epoch 22/50, iter: 300/3401, mean loss: 0.02571004027875915\n",
      "Epoch 22/50, iter: 400/3401, mean loss: 0.03080741747789034\n",
      "Epoch 22/50, iter: 500/3401, mean loss: 0.041090488413284734\n",
      "Epoch 22/50, iter: 600/3401, mean loss: 0.03290339694157907\n",
      "Epoch 22/50, iter: 700/3401, mean loss: 0.004344270735234801\n",
      "Epoch 22/50, iter: 800/3401, mean loss: 0.0032854925306736505\n",
      "Epoch 22/50, iter: 900/3401, mean loss: 0.004519592490878211\n",
      "Epoch 22/50, iter: 1000/3401, mean loss: 0.15046327870629952\n",
      "Epoch 22/50, iter: 1100/3401, mean loss: 0.22894357071107152\n",
      "Epoch 22/50, iter: 1200/3401, mean loss: 0.13301028261844522\n",
      "Epoch 22/50, iter: 1300/3401, mean loss: 0.06431082134409849\n",
      "Epoch 22/50, iter: 1400/3401, mean loss: 0.03337039476041369\n",
      "Epoch 22/50, iter: 1500/3401, mean loss: 0.04413704206982892\n",
      "Epoch 22/50, iter: 1600/3401, mean loss: 0.022840237256654062\n",
      "Epoch 22/50, iter: 1700/3401, mean loss: 0.022761457815651233\n",
      "Epoch 22/50, iter: 1800/3401, mean loss: 0.26288950393696325\n",
      "Epoch 22/50, iter: 1900/3401, mean loss: 0.2808332216984127\n",
      "Epoch 22/50, iter: 2000/3401, mean loss: 0.11469019495425528\n",
      "Epoch 22/50, iter: 2100/3401, mean loss: 0.03738710139127051\n",
      "Epoch 22/50, iter: 2200/3401, mean loss: 0.0447329973392597\n",
      "Epoch 22/50, iter: 2300/3401, mean loss: 0.025940730941767073\n",
      "Epoch 22/50, iter: 2400/3401, mean loss: 0.02471248287459872\n",
      "Epoch 22/50, iter: 2500/3401, mean loss: 0.0158703830268405\n",
      "Epoch 22/50, iter: 2600/3401, mean loss: 0.40840582608758597\n",
      "Epoch 22/50, iter: 2700/3401, mean loss: 0.22869251388066913\n",
      "Epoch 22/50, iter: 2800/3401, mean loss: 0.06824575861021004\n",
      "Epoch 22/50, iter: 2900/3401, mean loss: 0.03873470254435233\n",
      "Epoch 22/50, iter: 3000/3401, mean loss: 0.034262299814454306\n",
      "Epoch 22/50, iter: 3100/3401, mean loss: 0.02994294650389264\n",
      "Epoch 22/50, iter: 3200/3401, mean loss: 0.06289232246206587\n",
      "Epoch 22/50, iter: 3300/3401, mean loss: 0.00893444302079434\n",
      "Epoch 22/50, iter: 3400/3401, mean loss: 0.01361093483583339\n",
      "Epoch 23/50, iter: 100/3401, mean loss: 0.29943629930654425\n",
      "Epoch 23/50, iter: 200/3401, mean loss: 0.0471301395257467\n",
      "Epoch 23/50, iter: 300/3401, mean loss: 0.02510315558836737\n",
      "Epoch 23/50, iter: 400/3401, mean loss: 0.016088009737118228\n",
      "Epoch 23/50, iter: 500/3401, mean loss: 0.06237545100036868\n",
      "Epoch 23/50, iter: 600/3401, mean loss: 0.03344091110391332\n",
      "Epoch 23/50, iter: 700/3401, mean loss: 0.00539438962870463\n",
      "Epoch 23/50, iter: 800/3401, mean loss: 0.002077313049842431\n",
      "Epoch 23/50, iter: 900/3401, mean loss: 0.012476957233005521\n",
      "Epoch 23/50, iter: 1000/3401, mean loss: 0.13429363492065705\n",
      "Epoch 23/50, iter: 1100/3401, mean loss: 0.2106914378813781\n",
      "Epoch 23/50, iter: 1200/3401, mean loss: 0.11941636668800129\n",
      "Epoch 23/50, iter: 1300/3401, mean loss: 0.04899456648337264\n",
      "Epoch 23/50, iter: 1400/3401, mean loss: 0.03501679736647418\n",
      "Epoch 23/50, iter: 1500/3401, mean loss: 0.021307345264299896\n",
      "Epoch 23/50, iter: 1600/3401, mean loss: 0.03162198352755333\n",
      "Epoch 23/50, iter: 1700/3401, mean loss: 0.016316065606350207\n",
      "Epoch 23/50, iter: 1800/3401, mean loss: 0.24015168742424237\n",
      "Epoch 23/50, iter: 1900/3401, mean loss: 0.1826313681153215\n",
      "Epoch 23/50, iter: 2000/3401, mean loss: 0.1058647240962506\n",
      "Epoch 23/50, iter: 2100/3401, mean loss: 0.03788126352062591\n",
      "Epoch 23/50, iter: 2200/3401, mean loss: 0.04105745333296909\n",
      "Epoch 23/50, iter: 2300/3401, mean loss: 0.03173682877526062\n",
      "Epoch 23/50, iter: 2400/3401, mean loss: 0.029843242196433445\n",
      "Epoch 23/50, iter: 2500/3401, mean loss: 0.006959886612877853\n",
      "Epoch 23/50, iter: 2600/3401, mean loss: 0.4731403793754454\n",
      "Epoch 23/50, iter: 2700/3401, mean loss: 0.16822418833195116\n",
      "Epoch 23/50, iter: 2800/3401, mean loss: 0.06003843634504847\n",
      "Epoch 23/50, iter: 2900/3401, mean loss: 0.030663179251137082\n",
      "Epoch 23/50, iter: 3000/3401, mean loss: 0.05151317544680552\n",
      "Epoch 23/50, iter: 3100/3401, mean loss: 0.03778411036258916\n",
      "Epoch 23/50, iter: 3200/3401, mean loss: 0.01791958748850341\n",
      "Epoch 23/50, iter: 3300/3401, mean loss: 0.0068681848845908175\n",
      "Epoch 23/50, iter: 3400/3401, mean loss: 0.007043490074575871\n",
      "Epoch 24/50, iter: 100/3401, mean loss: 0.18462678860043524\n",
      "Epoch 24/50, iter: 200/3401, mean loss: 0.0464289492123703\n",
      "Epoch 24/50, iter: 300/3401, mean loss: 0.03690858664062716\n",
      "Epoch 24/50, iter: 400/3401, mean loss: 0.013882027702586654\n",
      "Epoch 24/50, iter: 500/3401, mean loss: 0.02909741836229301\n",
      "Epoch 24/50, iter: 600/3401, mean loss: 0.022023708730425254\n",
      "Epoch 24/50, iter: 700/3401, mean loss: 0.004391802633371782\n",
      "Epoch 24/50, iter: 800/3401, mean loss: 0.004260754461376308\n",
      "Epoch 24/50, iter: 900/3401, mean loss: 0.003212892835058483\n",
      "Epoch 24/50, iter: 1000/3401, mean loss: 0.13368296741326674\n",
      "Epoch 24/50, iter: 1100/3401, mean loss: 0.23185429796121754\n",
      "Epoch 24/50, iter: 1200/3401, mean loss: 0.131400463885534\n",
      "Epoch 24/50, iter: 1300/3401, mean loss: 0.05192417512021393\n",
      "Epoch 24/50, iter: 1400/3401, mean loss: 0.024389495848083698\n",
      "Epoch 24/50, iter: 1500/3401, mean loss: 0.014601167858497419\n",
      "Epoch 24/50, iter: 1600/3401, mean loss: 0.03347619365759428\n",
      "Epoch 24/50, iter: 1700/3401, mean loss: 0.020732671778730775\n",
      "Epoch 24/50, iter: 1800/3401, mean loss: 0.218609525964996\n",
      "Epoch 24/50, iter: 1900/3401, mean loss: 0.24902793771402912\n",
      "Epoch 24/50, iter: 2000/3401, mean loss: 0.10009219967888726\n",
      "Epoch 24/50, iter: 2100/3401, mean loss: 0.0362435639481248\n",
      "Epoch 24/50, iter: 2200/3401, mean loss: 0.0893903183702004\n",
      "Epoch 24/50, iter: 2300/3401, mean loss: 0.046554091033297024\n",
      "Epoch 24/50, iter: 2400/3401, mean loss: 0.018079739550690165\n",
      "Epoch 24/50, iter: 2500/3401, mean loss: 0.02574483155315704\n",
      "Epoch 24/50, iter: 2600/3401, mean loss: 0.40774070551030184\n",
      "Epoch 24/50, iter: 2700/3401, mean loss: 0.1711715783204272\n",
      "Epoch 24/50, iter: 2800/3401, mean loss: 0.048989102514724436\n",
      "Epoch 24/50, iter: 2900/3401, mean loss: 0.03537202602774528\n",
      "Epoch 24/50, iter: 3000/3401, mean loss: 0.03149365980882127\n",
      "Epoch 24/50, iter: 3100/3401, mean loss: 0.022269094800828382\n",
      "Epoch 24/50, iter: 3200/3401, mean loss: 0.022287951125917544\n",
      "Epoch 24/50, iter: 3300/3401, mean loss: 0.006488674773814154\n",
      "Epoch 24/50, iter: 3400/3401, mean loss: 0.01146074192628518\n",
      "Epoch 25/50, iter: 100/3401, mean loss: 0.25674738735516256\n",
      "Epoch 25/50, iter: 200/3401, mean loss: 0.060751294270448855\n",
      "Epoch 25/50, iter: 300/3401, mean loss: 0.021019398407118004\n",
      "Epoch 25/50, iter: 400/3401, mean loss: 0.014584869976917502\n",
      "Epoch 25/50, iter: 500/3401, mean loss: 0.03259691368226754\n",
      "Epoch 25/50, iter: 600/3401, mean loss: 0.026987353221300126\n",
      "Epoch 25/50, iter: 700/3401, mean loss: 0.006488586683652784\n",
      "Epoch 25/50, iter: 800/3401, mean loss: 0.0036983737938598438\n",
      "Epoch 25/50, iter: 900/3401, mean loss: 0.004999135280516498\n",
      "Epoch 25/50, iter: 1000/3401, mean loss: 0.10346546625690564\n",
      "Epoch 25/50, iter: 1100/3401, mean loss: 0.19886105461758233\n",
      "Epoch 25/50, iter: 1200/3401, mean loss: 0.12422010002774642\n",
      "Epoch 25/50, iter: 1300/3401, mean loss: 0.09208587110513236\n",
      "Epoch 25/50, iter: 1400/3401, mean loss: 0.05046139441725764\n",
      "Epoch 25/50, iter: 1500/3401, mean loss: 0.023653447779713056\n",
      "Epoch 25/50, iter: 1600/3401, mean loss: 0.021776632241485457\n",
      "Epoch 25/50, iter: 1700/3401, mean loss: 0.026145055969784396\n",
      "Epoch 25/50, iter: 1800/3401, mean loss: 0.2034356345659349\n",
      "Epoch 25/50, iter: 1900/3401, mean loss: 0.20701394581390217\n",
      "Epoch 25/50, iter: 2000/3401, mean loss: 0.1095427092738538\n",
      "Epoch 25/50, iter: 2100/3401, mean loss: 0.04430020672695207\n",
      "Epoch 25/50, iter: 2200/3401, mean loss: 0.03751684687019861\n",
      "Epoch 25/50, iter: 2300/3401, mean loss: 0.02929305201166102\n",
      "Epoch 25/50, iter: 2400/3401, mean loss: 0.018020561535170856\n",
      "Epoch 25/50, iter: 2500/3401, mean loss: 0.025224338775473427\n",
      "Epoch 25/50, iter: 2600/3401, mean loss: 0.40406783081864717\n",
      "Epoch 25/50, iter: 2700/3401, mean loss: 0.17286884350315632\n",
      "Epoch 25/50, iter: 2800/3401, mean loss: 0.04848506031594752\n",
      "Epoch 25/50, iter: 2900/3401, mean loss: 0.029031007386303428\n",
      "Epoch 25/50, iter: 3000/3401, mean loss: 0.021503352658908455\n",
      "Epoch 25/50, iter: 3100/3401, mean loss: 0.018804480474054017\n",
      "Epoch 25/50, iter: 3200/3401, mean loss: 0.024262632483678315\n",
      "Epoch 25/50, iter: 3300/3401, mean loss: 0.011455326355311008\n",
      "Epoch 25/50, iter: 3400/3401, mean loss: 0.011288476932795959\n",
      "Epoch 26/50, iter: 100/3401, mean loss: 0.24562392118528806\n",
      "Epoch 26/50, iter: 200/3401, mean loss: 0.06237559628699728\n",
      "Epoch 26/50, iter: 300/3401, mean loss: 0.03328456383742846\n",
      "Epoch 26/50, iter: 400/3401, mean loss: 0.04558388853311445\n",
      "Epoch 26/50, iter: 500/3401, mean loss: 0.047404847583967465\n",
      "Epoch 26/50, iter: 600/3401, mean loss: 0.03704834581482856\n",
      "Epoch 26/50, iter: 700/3401, mean loss: 0.006251394762982301\n",
      "Epoch 26/50, iter: 800/3401, mean loss: 0.0030394624640106118\n",
      "Epoch 26/50, iter: 900/3401, mean loss: 0.009714846617622114\n",
      "Epoch 26/50, iter: 1000/3401, mean loss: 0.07017214957184677\n",
      "Epoch 26/50, iter: 1100/3401, mean loss: 0.19732755699315022\n",
      "Epoch 26/50, iter: 1200/3401, mean loss: 0.1208316762313234\n",
      "Epoch 26/50, iter: 1300/3401, mean loss: 0.0391960314573015\n",
      "Epoch 26/50, iter: 1400/3401, mean loss: 0.03978149361009642\n",
      "Epoch 26/50, iter: 1500/3401, mean loss: 0.024152522635372976\n",
      "Epoch 26/50, iter: 1600/3401, mean loss: 0.019792767338644383\n",
      "Epoch 26/50, iter: 1700/3401, mean loss: 0.015001245406847942\n",
      "Epoch 26/50, iter: 1800/3401, mean loss: 0.1998410277173492\n",
      "Epoch 26/50, iter: 1900/3401, mean loss: 0.2413693487623084\n",
      "Epoch 26/50, iter: 2000/3401, mean loss: 0.11608072167462183\n",
      "Epoch 26/50, iter: 2100/3401, mean loss: 0.04019865430301706\n",
      "Epoch 26/50, iter: 2200/3401, mean loss: 0.03410710146036209\n",
      "Epoch 26/50, iter: 2300/3401, mean loss: 0.024565113272316096\n",
      "Epoch 26/50, iter: 2400/3401, mean loss: 0.023394066350139155\n",
      "Epoch 26/50, iter: 2500/3401, mean loss: 0.007172212275338694\n",
      "Epoch 26/50, iter: 2600/3401, mean loss: 0.3274734977724439\n",
      "Epoch 26/50, iter: 2700/3401, mean loss: 0.17761168423883647\n",
      "Epoch 26/50, iter: 2800/3401, mean loss: 0.05763239116708064\n",
      "Epoch 26/50, iter: 2900/3401, mean loss: 0.03193574998415556\n",
      "Epoch 26/50, iter: 3000/3401, mean loss: 0.030746865883025123\n",
      "Epoch 26/50, iter: 3100/3401, mean loss: 0.03796585779991293\n",
      "Epoch 26/50, iter: 3200/3401, mean loss: 0.011463065615953134\n",
      "Epoch 26/50, iter: 3300/3401, mean loss: 0.01674949476294479\n",
      "Epoch 26/50, iter: 3400/3401, mean loss: 0.02816394626785204\n",
      "Epoch 27/50, iter: 100/3401, mean loss: 0.22004846903000724\n",
      "Epoch 27/50, iter: 200/3401, mean loss: 0.054283505449338916\n",
      "Epoch 27/50, iter: 300/3401, mean loss: 0.04089199763668176\n",
      "Epoch 27/50, iter: 400/3401, mean loss: 0.011969816981831655\n",
      "Epoch 27/50, iter: 500/3401, mean loss: 0.0254171630720181\n",
      "Epoch 27/50, iter: 600/3401, mean loss: 0.03166582519334938\n",
      "Epoch 27/50, iter: 700/3401, mean loss: 0.005615026081617351\n",
      "Epoch 27/50, iter: 800/3401, mean loss: 0.0197640635415587\n",
      "Epoch 27/50, iter: 900/3401, mean loss: 0.05199044696670924\n",
      "Epoch 27/50, iter: 1000/3401, mean loss: 0.10561800272791842\n",
      "Epoch 27/50, iter: 1100/3401, mean loss: 0.17847574967433333\n",
      "Epoch 27/50, iter: 1200/3401, mean loss: 0.16115405337730976\n",
      "Epoch 27/50, iter: 1300/3401, mean loss: 0.08176497240244089\n",
      "Epoch 27/50, iter: 1400/3401, mean loss: 0.04702320307456759\n",
      "Epoch 27/50, iter: 1500/3401, mean loss: 0.020677162380973044\n",
      "Epoch 27/50, iter: 1600/3401, mean loss: 0.020565551996852525\n",
      "Epoch 27/50, iter: 1700/3401, mean loss: 0.04637474455113456\n",
      "Epoch 27/50, iter: 1800/3401, mean loss: 0.257931356955748\n",
      "Epoch 27/50, iter: 1900/3401, mean loss: 0.21598686455514327\n",
      "Epoch 27/50, iter: 2000/3401, mean loss: 0.1213000903567908\n",
      "Epoch 27/50, iter: 2100/3401, mean loss: 0.0356546655021377\n",
      "Epoch 27/50, iter: 2200/3401, mean loss: 0.04232032070343848\n",
      "Epoch 27/50, iter: 2300/3401, mean loss: 0.04219040761890426\n",
      "Epoch 27/50, iter: 2400/3401, mean loss: 0.0344976631521968\n",
      "Epoch 27/50, iter: 2500/3401, mean loss: 0.007500341761717948\n",
      "Epoch 27/50, iter: 2600/3401, mean loss: 0.4083215670674122\n",
      "Epoch 27/50, iter: 2700/3401, mean loss: 0.2075671770381905\n",
      "Epoch 27/50, iter: 2800/3401, mean loss: 0.06312880226076231\n",
      "Epoch 27/50, iter: 2900/3401, mean loss: 0.043608592554401186\n",
      "Epoch 27/50, iter: 3000/3401, mean loss: 0.049834688753350065\n",
      "Epoch 27/50, iter: 3100/3401, mean loss: 0.03901141419499652\n",
      "Epoch 27/50, iter: 3200/3401, mean loss: 0.016806027465151488\n",
      "Epoch 27/50, iter: 3300/3401, mean loss: 0.012027074137623437\n",
      "Epoch 27/50, iter: 3400/3401, mean loss: 0.02521551972584387\n",
      "Epoch 28/50, iter: 100/3401, mean loss: 0.3548762859522685\n",
      "Epoch 28/50, iter: 200/3401, mean loss: 0.05194614278824247\n",
      "Epoch 28/50, iter: 300/3401, mean loss: 0.02918987693134568\n",
      "Epoch 28/50, iter: 400/3401, mean loss: 0.012492256373060968\n",
      "Epoch 28/50, iter: 500/3401, mean loss: 0.021209463376642076\n",
      "Epoch 28/50, iter: 600/3401, mean loss: 0.03159791226424947\n",
      "Epoch 28/50, iter: 700/3401, mean loss: 0.006993101718772721\n",
      "Epoch 28/50, iter: 800/3401, mean loss: 0.002062525326398017\n",
      "Epoch 28/50, iter: 900/3401, mean loss: 0.013012231972867739\n",
      "Epoch 28/50, iter: 1000/3401, mean loss: 0.15064642836108702\n",
      "Epoch 28/50, iter: 1100/3401, mean loss: 0.2661255913515197\n",
      "Epoch 28/50, iter: 1200/3401, mean loss: 0.1885649234226861\n",
      "Epoch 28/50, iter: 1300/3401, mean loss: 0.056844662735797157\n",
      "Epoch 28/50, iter: 1400/3401, mean loss: 0.042445986982183964\n",
      "Epoch 28/50, iter: 1500/3401, mean loss: 0.025519107083579514\n",
      "Epoch 28/50, iter: 1600/3401, mean loss: 0.0361464038076906\n",
      "Epoch 28/50, iter: 1700/3401, mean loss: 0.019302966088767803\n",
      "Epoch 28/50, iter: 1800/3401, mean loss: 0.31770259365351367\n",
      "Epoch 28/50, iter: 1900/3401, mean loss: 0.281198289654676\n",
      "Epoch 28/50, iter: 2000/3401, mean loss: 0.10935619501168731\n",
      "Epoch 28/50, iter: 2100/3401, mean loss: 0.044826388950987166\n",
      "Epoch 28/50, iter: 2200/3401, mean loss: 0.05513704923370824\n",
      "Epoch 28/50, iter: 2300/3401, mean loss: 0.08309875555631833\n",
      "Epoch 28/50, iter: 2400/3401, mean loss: 0.04038229385226486\n",
      "Epoch 28/50, iter: 2500/3401, mean loss: 0.017665937815982316\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-2-17406c516246>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     83\u001B[0m         \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprediction\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mview\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     84\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 85\u001B[1;33m         \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     86\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     87\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\ars86\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    243\u001B[0m                 \u001B[0mcreate_graph\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    244\u001B[0m                 inputs=inputs)\n\u001B[1;32m--> 245\u001B[1;33m         \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    246\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    247\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\ars86\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\autograd\\__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    143\u001B[0m         \u001B[0mretain_graph\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    144\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 145\u001B[1;33m     Variable._execution_engine.run_backward(\n\u001B[0m\u001B[0;32m    146\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    147\u001B[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001B[1;32mc:\\users\\ars86\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\wandb\\wandb_torch.py\u001B[0m in \u001B[0;36m<lambda>\u001B[1;34m(grad)\u001B[0m\n\u001B[0;32m    284\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlog_tensor_stats\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgrad\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    285\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 286\u001B[1;33m         \u001B[0mhandle\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvar\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mregister_hook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0mgrad\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0m_callback\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgrad\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlog_track\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    287\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_hook_handles\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mhandle\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    288\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mhandle\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "use_wandb = True\n",
    "\n",
    "lr = 0.0005\n",
    "embedding_size = 100\n",
    "hidden_size = 100\n",
    "epochs_cnt = 50\n",
    "embeddings = \"random\"\n",
    "lstm_layers = 1\n",
    "dropout = 0.5\n",
    "task = \"text2company\" # \"text2sentiment\"\n",
    "use_company_info = True\n",
    "preprocessing = \"tutorial\"\n",
    "use_stop_words = True\n",
    "\n",
    "get_dataloaders = get_twit_company_dataloaders if task == \"text2company\" else\\\n",
    "get_twit_sentiment_dataloaders if not use_company_info else get_twit_company_sentiment_dataloaders\n",
    "\n",
    "dataset_train, dataloader_train, dataset_test, dataloader_test = get_twit_company_dataloaders(embedding_dim=embedding_size, embedding=embeddings, preprocessing=preprocessing, use_stop_words=use_stop_words)\n",
    "\n",
    "model = LSTMTwitClassifier(4, embedding_dim=embedding_size, hidden_dim=hidden_size, dropout=dropout, lstm_layers=lstm_layers)\n",
    "\n",
    "if use_wandb:\n",
    "    import wandb\n",
    "\n",
    "    wandb.init(project=task + '_twit_classification', entity='ars860')\n",
    "\n",
    "    config = wandb.config\n",
    "    config.loss = \"BCE\"\n",
    "    config.optimizer = \"Adam\"\n",
    "    config.learning_rate = lr\n",
    "    config.hidden_size = hidden_size\n",
    "    config.embedding_size = embedding_size\n",
    "    config.embeddings = embeddings\n",
    "    config.epochs = epochs_cnt\n",
    "    config.dropout = dropout\n",
    "    config.lstm_layers = lstm_layers\n",
    "    config.stem = \"snowballstemmer\"\n",
    "    config.preprocessing = preprocessing\n",
    "    config.use_stop_words = use_stop_words\n",
    "\n",
    "    if task == \"text2sentiment\":\n",
    "        config.use_company_info = use_company_info\n",
    "\n",
    "    wandb.watch(model)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def loss_on_test():\n",
    "    correct = 0\n",
    "    losses = np.zeros(len(dataloader_test))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i, (*args, target) in enumerate(dataloader_test):\n",
    "            prediction = model(*args)\n",
    "            prediction = F.softmax(prediction, dim=0)\n",
    "\n",
    "            losses[i] = F.binary_cross_entropy(prediction, target.view(-1))\n",
    "            if torch.argmax(prediction) == torch.argmax(target):\n",
    "                correct += 1\n",
    "\n",
    "            # predictions_cnt[torch.argmax(prediction)] += 1\n",
    "\n",
    "            # if i % 100 == 0:\n",
    "            #     print(f\"Iter: {i}/{len(dataloader_test)}\")\n",
    "\n",
    "    model.train()\n",
    "    if use_wandb:\n",
    "        wandb.log({\"test_loss\": np.mean(losses), \"test_accuracy\": correct / len(dataloader_test)})\n",
    "\n",
    "losses = np.empty(100)\n",
    "model.train()\n",
    "for epoch in range(epochs_cnt):\n",
    "    epoch_loss = np.zeros(len(dataloader_train))\n",
    "\n",
    "    for i, (*args, target) in enumerate(dataloader_train):\n",
    "        model.zero_grad()\n",
    "\n",
    "        prediction = model(*args)\n",
    "        prediction = F.softmax(prediction, dim=0)\n",
    "\n",
    "        loss = criterion(prediction, target.view(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss = loss.detach().item()\n",
    "        losses[i % 100] = loss\n",
    "        epoch_loss[i] = loss\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{epochs_cnt}, iter: {i + 1}/{len(dataloader_train)}, mean loss: {np.mean(losses)}\")\n",
    "            if use_wandb:\n",
    "                wandb.log({\"loss\": np.mean(losses)})\n",
    "\n",
    "    if use_wandb:\n",
    "        wandb.log({\"epoch_loss\": np.mean(epoch_loss)})\n",
    "        loss_on_test()\n",
    "\n",
    "# [model.get_word_embedding(word) for word in \"hello_world\".split(' ')]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "print(\"Testing on train\")\n",
    "\n",
    "correct = 0\n",
    "predictions_cnt = [0, 0, 0, 0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (*args, target) in enumerate(dataloader_train):\n",
    "        prediction = model(*args)\n",
    "        prediction = F.softmax(prediction, dim=0)\n",
    "\n",
    "        if torch.argmax(prediction) == torch.argmax(target):\n",
    "            correct += 1\n",
    "\n",
    "        predictions_cnt[torch.argmax(prediction)] += 1\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iter: {i}/{len(dataloader_train)}\")\n",
    "\n",
    "print(f\"Accuracy {correct / len(dataloader_train)}\")\n",
    "\n",
    "if use_wandb:\n",
    "    wandb.run.summary.train_accuracy = correct / len(dataloader_train)\n",
    "    wandb.run.summary.classified_as = {\n",
    "        \"apple\": predictions_cnt[0],\n",
    "        \"google\": predictions_cnt[1],\n",
    "        \"microsoft\": predictions_cnt[2],\n",
    "        \"twitter\": predictions_cnt[3]\n",
    "    }\n",
    "    wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Testing on test\")\n",
    "\n",
    "correct = 0\n",
    "predictions_cnt = [0, 0, 0, 0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (*args, target) in enumerate(dataloader_test):\n",
    "        prediction = model(*args)\n",
    "        prediction = F.softmax(prediction, dim=0)\n",
    "\n",
    "        if torch.argmax(prediction) == torch.argmax(target):\n",
    "            correct += 1\n",
    "\n",
    "        predictions_cnt[torch.argmax(prediction)] += 1\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iter: {i}/{len(dataloader_test)}\")\n",
    "\n",
    "print(f\"Accuracy {correct / len(dataloader_test)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}